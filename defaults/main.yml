---
# use local installation media instead of configured repositories (CentOS only)
use_local_media: false

# user used for authorizing cluster nodes
cluster_user: hacluster
cluster_user_pass: testtest
# group to which cluster user belongs (should be 'haclient')
cluster_group: haclient

# name of the cluster
cluster_name: pacemaker

# configuration of firewall for clustering, NOTE in RHEL/Centos 6 this replaces iptables configuration file!
cluster_firewall: true

# enable cluster on boot on normal (not pacemaker_remote) nodes
cluster_enable_service: true

# configure cluster with fence_xvm fencing device ?
# this will copy key defined by 'fence_xvm_key' to nodes and add fencing devices to cluster
# NOTE: you need to define 'vm_name' in the inventory for each cluster node
cluster_configure_fence_xvm: true

# location of the fence_xvm.key file
fence_xvm_key: '/etc/cluster/fence_xvm.key'

# configure cluster with fence_kdump fencing device ?
cluster_configure_fence_kdump: false

# configure cluster with fence_vmware_soap/fence_vmware_rest fencing device?
# You must provide IP/hostname of vCenter/hypervisor and username/password that is able to start/stop VMs for this cluster
cluster_configure_fence_vmware_soap: false
cluster_configure_fence_vmware_rest: false
#fence_vmware_ipaddr: ''
#fence_vmware_login: ''
#fence_vmware_passwd: ''
# by default we use encrypted configuration (ssl=1) without validating certificates (ssl_insecure=1)
fence_vmware_options: 'ssl="1" ssl_insecure="1"'
# NOTE: Only one of fence_vmware_soap/fence_vmware_rest can be configured as stonith devices share same name.

# custom fence device configuration variable which allows you to define your own fence devices
# for proper options check examples below
#
#cluster_fence_config:
#  fence_device_1:
#    fence_type: 'fence_vmware_soap'
#    fence_options: 'pcmk_host_map="fastvm-1:vm_name_on_hypevisor1" ipaddr="vcenter.hostname" login="root" passwd="testest" ssl="1" ssl_insecure="1" op monitor interval=30s'
#  fence_device_2:
#    fence_type: 'fence_xvm'
#    fence_options: 'pcmk_host_map="fastvm-2:vm_name_n_hypervisor2" op monitor interval=30s'

# How to map fence devices to cluster nodes?
# by default for every cluster node a separate stonith devices is created ('one-device-per-node').
# Some fence agents can fence multiple nodes using same stonith device ('one-device-per-cluster')
# and can have trouble when using multiple devices due to same user login count limits.
# available options:
# - 'one-device-per-node' - (default) - one stonith device per cluster node is created
# - 'one-device-per-cluster' - (on supported fence agents) - only one cluster-wide stonith device is created for all nodes, supported fence agents: 'fence_vmware_rest', 'fence_vmware_soap', 'fence_xvm', 'fence_kdump'
cluster_configure_stonith_style: 'one-device-per-node'

# (RHEL only) enable repositories(channels) containing the packages needed.
# by default only High Availability is enabled
# E4S repositories are High Availability only
enable_repos: true
enable_eus_repos: false
enable_e4s_repos: false
enable_beta_repos: false

# (RHEL only) repos_type:
# - ha - High Availability channel
# - rs - Resilient Storage channel
repos_type: 'ha'

# (RHEL only) custom_repository
# Enable custom arbitrary repository with needed packages
# custom_repository: ''

# Enable/Disable PCSD web GUI
# available options are:
# 'nochange' (default) - don't change the PCSD GUI settings
# true - enable PCSD GUI
# false - disable PCSD GUI
enable_pcsd_gui: 'nochange'

# Cluster transport protocol
# avaialble options:
# 'default' (default) - use the OS default for cluster transport
# 'udp' - UDP multicast
# 'udpu' - UDP unicast
cluster_transport: 'default'

# Allow adding nodes to existing cluster
allow_cluster_expansion: false

# Cluster network interface. If specified the role will map hosts to IPv4 addresses from this interface.
# By default the IPv4 addresses from `ansible_default_ipv4` are used. For exmaple to use IPv4 addresses
# from interface `ens8` use `cluster_net_iface: 'ens8'`. Interface must exists on all cluster nodes.
cluster_net_iface: ''

#Redundant network interface. If specified the role will setup a corosync redundant ring using the default IPv4 from this interface.
#Interface must exist on all cluster nodes.
rrp_interface: ''

# Whether to add hosts to /etc/hosts.
cluster_etc_hosts: true

# Which Ansible fact to use as the hostname of cluster nodes.
cluster_hostname_fact: "ansible_hostname"

# Whether the node should be setup as a remote pacemaker node.
cluster_node_is_remote: false
